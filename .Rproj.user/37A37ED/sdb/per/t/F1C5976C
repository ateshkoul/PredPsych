{
    "collab_server" : "",
    "contents" : "#' Generic Classification Analyses\n#'\n#' function for performing generic classification Analysis\n#' \n#' @param Data            (dataframe) dataframe of the data\n#' @param predictorCol    (numeric) column number that contains the variable to be predicted\n#' @param selectedCols    (optional) (numeric) all the columns of data that would be used either as predictor or as feature\n#' @param classifierName  (optional) (string) name of the classifier to be used\n#' @param genclassifier   (optional) (function or string) a classifier function or a name (e.g. Classifier.svm)\n#' @param ranges          (optional) (list)  ranges for tuning support vector machine\n#' @param tune            (optional) (logical) whether tuning of svm parameters should be performed or not\n#' @param cost            (optional) (numeric) regularization parameter of svm\n#' @param gamma           (optional) (numeric)  rbf kernel parameter\n#'\n#' @return Outputs Crossvalidation accuracy \\code{acc} and    Test accuracy \\code{accTest}\n#'\n#'\n#'\n#'\n#'@author\n#'Atesh Koul, C'MON group, Istituto Italiano di technologia\n#'\n#'\\email{atesh.koul@@gmail.com}\nclassifyFun <- function(Data,predictorCol,selectedCols,ranges=NULL,tune=FALSE,cost=1,gamma=0.5,classifierName='svm',genclassifier=Classifier.svm,silent=FALSE,...){\n  # a simplistic k-fold crossvalidation\n  # For cross validation\n  library(e1071)\n  library(caret)\n  # dont use a constant set.seed with permutation testing\n  # u will get a constant accuracy!!\n  #set.seed(123)\n\n  if(missing(selectedCols))  selectedCols <- 1:length(names(Data))\n\n    # get the features\n  selectedColNames <- names(Data)[selectedCols]\n  # get feature columns without response\n  featureColNames <- selectedColNames[-match(names(Data)[predictorCol],selectedColNames)]\n  predictorColNames <- names(Data)[predictorCol]\n\n  Data = Data[,selectedCols]\n  Data[,predictorColNames] = factor(Data[,predictorColNames])\n\n  # if predictor has missing, remove those columns\n  if(sum(is.na(Data[,predictorColNames]))>0) Data <- Data[!is.na(Data[,predictorColNames]),]\n\n  if(tune) {\n    folds = 3\n    # use stratified cross validation instead\n    # divide the data into 3 parts:\n    # 1. for tuning parameters\n    # 2. for training model\n    # 3. for testing prediction (on a data that it has never seen ever)\n    #\n    # Keeping more for tuning\n    trainIndexOverall <- createFolds(Data[,predictorCol],list = FALSE,k = folds)\n    # leave first part for tuning the classifier\n    tuneTrainData <- Data[trainIndexOverall==1,]\n    ModelTrainData <- Data[trainIndexOverall==2,]\n    ModelTestData <- Data[trainIndexOverall==3,]\n    obj <- getTunedParam(tuneTrainData,predictorCol,classifierName,featureColNames,ranges)[\"best.parameters\"]\n\n  } else{\n    folds = 2\n    trainIndexOverall <- createFolds(Data[,predictorColNames],list = FALSE,k = folds)\n    ModelTrainData <- Data[trainIndexOverall==1,]\n    ModelTestData <- Data[trainIndexOverall==2,]\n    obj <- data.frame(gamma=gamma,cost=cost)\n  }\n\n  kFold <- 10\n  #initialising vectors\n  acc <- rep(NA,kFold)\n  accTest <- rep(NA,kFold)\n\n  trainIndexModel <- createFolds(ModelTrainData[,predictorColNames],list = FALSE,k = kFold)\n\n  if(!silent){\n    print('Begining k-fold Classification')\n  }\n  \n  \n  for (i in 1:kFold){\n    trainDataFold <- ModelTrainData[!trainIndexModel==i,]\n    testDataFold <- ModelTrainData[trainIndexModel==i,]\n\n    # the classifier has generic\n    # generic error function:\n    acc[i] = do.call(genclassifier,c(list(trainData=trainDataFold,testData=testDataFold,ModelTestData=ModelTestData,predictorColNames=predictorColNames,\n                                             featureColNames=featureColNames,expand.grid(obj),...)))[[\"acc\"]]\n    accTest[i] = do.call(genclassifier,c(list(trainData=trainDataFold,testData=testDataFold,ModelTestData=ModelTestData,predictorColNames=predictorColNames,\n                                                 featureColNames=featureColNames,expand.grid(obj),...)))[[\"accTest\"]]\n  }\n  if(!silent){\n    print(paste(\"Mean CV Accuracy\",signif(mean(acc),2)))\n    print(paste(\"Mean Test Accuracy\",signif(mean(accTest),2)))\n  }\n  \n\n\n#Results <- list(acc=acc,accTest=accTest)\n#return(Results)\nreturn(mean(accTest))\n\n}\n\n# get tuned parameters\ngetTunedParam <- function(tuneTrainData,predictorCol,classifierName,featureColNames,ranges=NULL){\n\n  classifierFun <- get(classifierName)\n  if(missing(featureColNames)) featureColNames <- 1:length(names(tuneTrainData))\n  # defaults\n  print('Begining Tuning Classifier')\n  # only in case of svm, suggest\n  if(classifierName==\"svm\"){\n    if (is.null(ranges)) ranges = list(gamma = 2^(-1:1), cost = 2^(2:4))\n    obj <- tune(classifierFun, train.y = tuneTrainData[,predictorCol],train.x = tuneTrainData[,featureColNames],\n                ranges = ranges,tunecontrol = tune.control(sampling = \"fix\"))\n  } else if(classifierName==\"knn3\") {\n    # chgoose range of k\n    if (is.null(ranges)) ranges = 1:10\n    obj <- tune.knn(y = tuneTrainData[,predictorCol],x = tuneTrainData[,featureColNames],\n                k = ranges,tunecontrol = tune.control(sampling = \"fix\"))\n  }\n  print(summary(obj))\n  plot(obj)\n\n\n  return(obj)\n\n}\n\n# classifier functions for knn3 and svm\nClassifier.svm <- function(trainData,testData,ModelTestData,predictorColNames,featureColNames,...){\n  model <- do.call('svm',c(list(y=trainData[,predictorColNames],x=trainData[,featureColNames]),...))\n  # test with train data\n  pred <- predict(model, testData[,featureColNames])\n  acc <- sum(1 * (pred==testData[,predictorColNames]))/length(pred)\n  predTest <- predict(model, ModelTestData[,featureColNames])\n  accTest <- sum(1 * (predTest==ModelTestData[,predictorColNames]))/length(predTest)\n  accList <- list(acc=acc,accTest=accTest)\n  return(accList)\n\n}\n\nClassifier.knn <- function(trainData,testData,ModelTestData,predictorColNames,featureColNames,obj,...){\n    model <- do.call(\"knn3\",c(list(y=trainData[,predictorColNames],x=trainData[,featureColNames]),expand.grid(obj$best.parameters),...))\n    # test with train data\n    pred <- predict(model, testData[,featureColNames],type=\"class\")\n    acc <- sum(1 * (pred==testData[,predictorColNames]))/length(pred)\n    predTest <- predict(model, ModelTestData[,featureColNames],type=\"class\")\n    accTest <- sum(1 * (predTest==ModelTestData[,predictorColNames]))/length(predTest)\n    accList <- list(acc=acc,accTest=accTest)\n    return(accList)\n}\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1469535336149.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1661384581",
    "id" : "F1C5976C",
    "lastKnownWriteTime" : 1469540956,
    "last_content_update" : 1469540956087,
    "path" : "D:/SVNWorkingDir/CMON/ReadingIntention/CommonScripts/PredPsych/R/classifyFun.R",
    "project_path" : "R/classifyFun.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}